{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "BCW_Model_Optimization_and_Deployment_Part 4.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinsocf/CapstoneProject/blob/main/BCW_Model_Optimization_and_Deployment_Part_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Y32Yxuhiyt"
      },
      "source": [
        "# Building Machine Learning Model to predict whether the cancer is benign or malignant on Breast Cancer Wisconsin Data Set !! Part 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TitJ_n0dhiyx"
      },
      "source": [
        "#### Hyperparameter Optimization : GridSearch with Cross Validation for tuning parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNxvspRUhiyy"
      },
      "source": [
        "#### Logistic Regression : Hyperparameter Tuning and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw-PnT3Uhiyz"
      },
      "source": [
        "# A parameter grid for Logistic Regression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression()\n",
        "penalty = ['l1', 'l2']\n",
        "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "#class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
        "solver = ['liblinear', 'saga']\n",
        "\n",
        "param_grid = dict(penalty=penalty,\n",
        "                  C=C,\n",
        "                  #class_weight=class_weight,\n",
        "                  solver=solver)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OGhkmPvj_lX"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "iCvzcsLohiy1",
        "outputId": "e1366cbd-1994-4e48-e712-f8837f5f1a0c"
      },
      "source": [
        "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3)\n",
        "\n",
        "# Here we go\n",
        "#start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
        "model.fit(X_train, y_train)\n",
        "#timer(start_time) # timing ends here for \"start_time\" variable\n",
        "best_model = best_mdl()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a8ccf9a097a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Here we go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#start_time = timer(None) # timing starts from this point for \"start_time\" variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukn3ukthhiy1"
      },
      "source": [
        "#### XGBoost : Hyperparameter Tuning and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHNwG93shiy2"
      },
      "source": [
        "# A parameter grid for XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "clf = XGBClassifier()\n",
        "# A parameter grid for XGBoost\n",
        "param_grid = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQYpu4rqhiy3"
      },
      "source": [
        "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3)\n",
        "\n",
        "# Here we go\n",
        "#start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
        "model.fit(X_train, y_train)\n",
        "#timer(start_time) # timing ends here for \"start_time\" variable\n",
        "best_model = best_mdl()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9wLDxzBhiy3"
      },
      "source": [
        "#### Gradient Boosting Classifier : Hyperparameter Tuning and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG5Tqel1hiy4"
      },
      "source": [
        "# A parameter grid for XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf = GradientBoostingClassifier(max_depth=3, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)\n",
        "# A parameter grid for GradientBoostingClassifier\n",
        "param_grid = {'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001], 'n_estimators':[100,250,500,750,1000,1250,1500,1750]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JXBGqSMhiy4"
      },
      "source": [
        "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3)\n",
        "\n",
        "# Here we go\n",
        "#start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
        "model.fit(X_train, y_train)\n",
        "#timer(start_time) # timing ends here for \"start_time\" variable\n",
        "best_model = best_mdl()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOSnNp0Uhiy5"
      },
      "source": [
        "#### Random Forest : Hyperparameter Tuning and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkVj8co9hiy5"
      },
      "source": [
        "# A parameter grid for XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import GradientBoostingClassifier\n",
        "clf = RandomForestClassifier()\n",
        "# A parameter grid for GradientBoostingClassifier\n",
        "param_grid = {'bootstrap': [True, False],\n",
        " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        " 'max_features': ['auto', 'sqrt'],\n",
        " 'min_samples_leaf': [1, 2, 4],\n",
        " 'min_samples_split': [2, 5, 10],\n",
        " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuuzYqkchiy5"
      },
      "source": [
        "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3)\n",
        "\n",
        "# Here we go\n",
        "#start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
        "model.fit(X_train, y_train)\n",
        "#timer(start_time) # timing ends here for \"start_time\" variable\n",
        "best_model = best_mdl()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QbQ6TS8hiy6"
      },
      "source": [
        "#### AUC, confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buPhAt9Uhiy6"
      },
      "source": [
        "y_predicted_test  = best_model.predict(X_test)\n",
        "y_probabilities_test = best_model.predict_proba(X_test)\n",
        "y_probabilities_success = y_probabilities_test[:, 1]\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(y_test, y_probabilities_success)\n",
        "print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
        "\n",
        "false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_probabilities_success)\n",
        "\n",
        "mse        = mean_squared_error(y_test, y_predicted_test)\n",
        "logloss    = log_loss(y_test, y_predicted_test)\n",
        "accuracy   = accuracy_score(y_test, y_predicted_test)\n",
        "precision  = precision_score(y_test, y_predicted_test, average='binary')\n",
        "recall     = recall_score(y_test, y_predicted_test, average='binary')\n",
        "F1         = f1_score(y_test, y_predicted_test)\n",
        "r2         = r2_score(y_test, y_predicted_test)\n",
        "auc        = roc_auc_score(y_test, y_predicted_test)\n",
        "cm         = confusion_matrix(y_test, y_predicted_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fgD9knEhiy6"
      },
      "source": [
        "best_model = best_mdl()\n",
        "Print_Model_Metrics()\n",
        "Plot_ROC_Precision_Recall()\n",
        "Plot_Confusion_Matrix()\n",
        "Plot_Predictor_Importance(True)\n",
        "#results_df = pd.DataFrame(data={'Observed':y_test, 'Predicted':y_predicted_test[:,1]})\n",
        "#results_df.to_csv('grid-search--outcome.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}